{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the captions from json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './DATA/word_features/captions.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58fdcf636242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mitoa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideoid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./DATA/word_features/captions.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitoa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './DATA/word_features/captions.pkl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, os.path\n",
    "import pickle\n",
    "\n",
    "train_val = json.load(open('videodatainfo_2017.json', 'r'))\n",
    "\n",
    "\n",
    "# combine all images and annotations together\n",
    "sentences = train_val['sentences']\n",
    "\n",
    "# for efficiency lets group annotations by video\n",
    "itoa = {}\n",
    "for s in sentences:\n",
    "    videoid_buf = s['video_id']\n",
    "    videoid = int(videoid_buf[5:])\n",
    "    if not videoid in itoa: itoa[videoid] = []\n",
    "    itoa[videoid].append(s)\n",
    "    \n",
    "output = open('./DATA/word_features/captions.pkl', 'wb')\n",
    "pickle.dump(itoa, output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilary functions to handle captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Functions to do the following:\n",
    "            * Create vocabulary\n",
    "            * Create dictionary mapping from word to word_id\n",
    "            * Map words in captions to word_ids\"\"\"\n",
    "\n",
    "def build_vocab(word_count_thresh):\n",
    "    \"\"\"Function to create vocabulary based on word count threshold.\n",
    "        Input:\n",
    "                word_count_thresh: Threshold to choose words to include to the vocabulary\n",
    "        Output:\n",
    "                vocabulary: Set of words in the vocabulary\"\"\"\n",
    "    \n",
    "    pkl_file = open('./DATA/word_features/captions.pkl', 'rb')\n",
    "    sentences = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "\n",
    "    unk_required = False\n",
    "    all_captions = []\n",
    "    word_counts = {}\n",
    "    for vid in sentences.keys():\n",
    "        for cid in range(0,20):\n",
    "            caption = sentences[vid][cid]['caption']\n",
    "            caption = '<BOS> ' + caption + ' <EOS>'\n",
    "            all_captions.append(caption)\n",
    "            for word in caption.split(' '):\n",
    "                if word in word_counts.keys():\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    for word in word_counts.keys():\n",
    "        if word_counts[word] < word_count_thresh:\n",
    "            word_counts.pop(word)\n",
    "            unk_required = True\n",
    "    return word_counts,unk_required\n",
    "\n",
    "def word_to_word_ids(word_counts,unk_required, vocab_size):\n",
    "    \"\"\"Function to map individual words to their id's.\n",
    "        Input:\n",
    "                word_counts: Dictionary with words mapped to their counts\n",
    "        Output:\n",
    "                word_to_id: Dictionary with words mapped to their id's. \n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    # Taking the most frequent vocab_size words\n",
    "    words = [word for word in word_counts.keys()]\n",
    "    values = [word_counts[word] for word in words]\n",
    "    sorted_indices = np.argsort(values)\n",
    "    words = np.array(words)\n",
    "    most_freq_words = words[sorted_indices[::-1][0:vocab_size]]\n",
    "    \n",
    "    id_to_word = [most_freq_words[i] for i in range(most_freq_words.shape[0])] \n",
    "    \n",
    "    #word2idx\n",
    "    word_to_id = {}\n",
    "    for i in range(len(id_to_word)):\n",
    "        word_to_id[id_to_word[i]] = i\n",
    "    \n",
    "    print(word_to_id['<EOS>'])\n",
    "    index = word_to_id['<EOS>']\n",
    "    word = id_to_word[0]\n",
    "    print(index,word)\n",
    "    \n",
    "    word_to_id['<EOS>'] = 0\n",
    "    id_to_word[0] = '<EOS>'\n",
    "    word_to_id[word] = index\n",
    "    id_to_word[index] = word\n",
    "    \n",
    "    return word_to_id,id_to_word\n",
    "\n",
    "def convert_caption(caption,word_to_id,max_caption_length):\n",
    "    \"\"\"Function to map each word in a caption to it's respective id and to retrieve caption masks\n",
    "        Input:\n",
    "                caption: Caption to convert to word_to_word_ids\n",
    "                word_to_id: Dictionary mapping words to their respective id's\n",
    "                max_caption_length: Maximum number of words allowed in a caption\n",
    "        Output:\n",
    "                caps: Captions with words mapped to word id's\n",
    "                cap_masks: Caption masks with 1's at positions of words and 0's at pad locations\"\"\"\n",
    "    caps,cap_masks = [],[]\n",
    "    if type(caption) == 'str':\n",
    "        caption = [caption] # if single caption, make it a list of captions of length one\n",
    "    for cap in caption:\n",
    "        cap = '<BOS> '+cap+' <EOS>'\n",
    "        nWords = cap.count(' ') + 1\n",
    "        if nWords >= max_caption_length:\n",
    "            carr = cap.split(' ')\n",
    "            carr = carr[0:(max_caption_length-2)]\n",
    "            cap  = ' '.join(carr)\n",
    "            cap  = cap + ' <EOS>'\n",
    "            nWords = cap.count(' ')+1\n",
    "        cap = cap + ' <EOS>'*(max_caption_length-nWords)\n",
    "        cap_masks.append([1.0]*nWords + [0.0]*(max_caption_length-nWords))\n",
    "        curr_cap = []\n",
    "        for word in cap.split(' '):\n",
    "            #print(word)\n",
    "            if word in word_to_id.keys():\n",
    "                curr_cap.append(word_to_id[word]) # word is present in chosen vocabulary\n",
    "            else:\n",
    "                curr_cap.append(word_to_id['<UNK>']) # word not present in chosen vocabulary\n",
    "        caps.append(curr_cap)\n",
    "        #print('Caption_Length:',len(caps[0]))\n",
    "    return np.array(caps),np.array(cap_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test  Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the list of the files we have extracted features\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "video_list = os.listdir('./DATA/features')\n",
    "videos = []\n",
    "for item in video_list:\n",
    "    videos.append(item.split('-')[0])\n",
    "\n",
    "video_train, video_test = train_test_split(videos, test_size=0.1, random_state=42)\n",
    "video_train, video_val = train_test_split(video_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Videos - 5890\n",
      "Testing Videos - 728\n",
      "Validation Videos - 655\n"
     ]
    }
   ],
   "source": [
    "print('Training Videos -', len(video_train))\n",
    "print('Testing Videos -', len(video_test))\n",
    "print('Validation Videos -', len(video_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxillary functions to handle model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 a\n",
      "5890 files processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import imageio\n",
    "import pickle\n",
    "np.random.seed(0)\n",
    "#Global initializations\n",
    "n_lstm_steps = 30\n",
    "DATA_DIR = './DATA/'\n",
    "VIDEO_DIR = DATA_DIR + 'features_3d/'\n",
    "YOUTUBE_CLIPS_DIR = DATA_DIR + 'videos/'\n",
    "TEXT_DIR = DATA_DIR+'word_features/'\n",
    "pkl_file = open('./DATA/word_features/captions.pkl', 'rb')\n",
    "sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "word_counts,unk_required = build_vocab(0)\n",
    "word2id,id2word = word_to_word_ids(word_counts,unk_required, len(word_counts.keys()))\n",
    "video_files = video_train\n",
    "val_files = video_val\n",
    "\n",
    "print (\"{0} files processed\".format(len(video_files)))\n",
    "\n",
    "def get_bias_vector():\n",
    "    \"\"\"Function to return the initialization for the bias vector\n",
    "       for mapping from hidden_dim to vocab_size.\n",
    "       Borrowed from neuraltalk by Andrej Karpathy\"\"\"\n",
    "    bias_init_vector = np.array([1.0*word_counts[id2word[i]] for i in range(len(id2word))])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector)\n",
    "    return bias_init_vector\n",
    "\n",
    "def fetch_data_batch(batch_size):\n",
    "    \"\"\"Function to fetch a batch of video features, captions and caption masks\n",
    "        Input:\n",
    "                batch_size: Size of batch to load\n",
    "        Output:\n",
    "                curr_vids: Features of the randomly selected batch of video_files\n",
    "                curr_caps: Ground truth (padded) captions for the selected videos\n",
    "                curr_masks: Mask for the pad locations in curr_caps\"\"\"\n",
    "    curr_batch_vids = np.random.choice(video_files,batch_size)\n",
    "    curr_vids = np.array([[np.load(VIDEO_DIR + vid+'-16' + '.npy')] for vid in curr_batch_vids])\n",
    "    captions = [np.random.choice(sentences[int(vid[5:])],1)[0]['caption'] for vid in curr_batch_vids]\n",
    "    curr_caps,curr_masks = convert_caption(captions,word2id,n_lstm_steps)\n",
    "    return curr_vids,curr_caps,curr_masks\n",
    "\n",
    "def fetch_data_batch_val(batch_size):\n",
    "    \"\"\"Function to fetch a batch of video features from the validation set and its captions.\n",
    "        Input:\n",
    "                batch_size: Size of batch to load\n",
    "        Output:\n",
    "                curr_vids: Features of the randomly selected batch of video_files\n",
    "                curr_caps: Ground truth (padded) captions for the selected videos\"\"\"\n",
    "\n",
    "    curr_batch_vids = np.random.choice(val_files,batch_size)\n",
    "    curr_vids = np.array([[np.load(VIDEO_DIR +vid+'-16' + '.npy')] for vid in curr_batch_vids])\n",
    "    captions = [np.random.choice(sentences[int(vid[5:])],1)[0]['caption'] for vid in curr_batch_vids]\n",
    "    curr_caps,curr_masks = convert_caption(captions,word2id,n_lstm_steps)\n",
    "    return curr_vids,curr_caps,curr_masks, curr_batch_vids\n",
    "\n",
    "\n",
    "def print_in_english(caption_idx):\n",
    "    \"\"\"Function to take a list of captions with words mapped to ids and\n",
    "        print the captions after mapping word indices back to words.\"\"\"\n",
    "    captions_english = [[id2word[word] for word in caption] for caption in caption_idx]\n",
    "    for i,caption in enumerate(captions_english):\n",
    "        if '<EOS>' in caption:\n",
    "            caption = caption[0:caption.index('<EOS>')]\n",
    "        print (str(i+1) + ' ' + ' '.join(caption))\n",
    "        print ('..................................................')\n",
    "\n",
    "def playVideo(video_urls):\n",
    "    video = imageio.get_reader(YOUTUBE_CLIPS_DIR + video_urls[0] + '.mp4','ffmpeg')\n",
    "    for frame in video:\n",
    "        fr = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        cv2.imshow('frame',fr)\n",
    "        if cv2.waitKey(40) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20001\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an attractive woman describes and demonstrates a cookie recipe'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_val = 'video3707'\n",
    "np.random.choice(sentences[int(tmp_val[5:])],1)[0]['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2048)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata = np.load(VIDEO_DIR+'video0-30-features.npy')\n",
    "tdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EOS> 1\n"
     ]
    }
   ],
   "source": [
    "print(id2word[0], word2id['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "#GLOBAL VARIABLE INITIALIZATIONS TO BUILD MODEL\n",
    "#n_steps = 30\n",
    "n_steps_vid = 1\n",
    "hidden_dim = 500\n",
    "frame_dim = 4096\n",
    "batch_size = 1\n",
    "vocab_size = len(word2id)\n",
    "bias_init_vector = get_bias_vector()\n",
    "n_steps_vocab = 30\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"This function creates weight matrices that transform:\n",
    "            * frames to caption dimension\n",
    "            * hidden state to vocabulary dimension\n",
    "            * creates word embedding matrix \"\"\"\n",
    "\n",
    "    print (\"Network config: \\nN_Steps: {}\\nHidden_dim:{}\\nFrame_dim:{}\\nBatch_size:{}\\nVocab_size:{}\\n\".format(n_steps_vid,\n",
    "                                                                                                    hidden_dim,\n",
    "                                                                                                    frame_dim,\n",
    "                                                                                                    batch_size,\n",
    "                                                                                                    vocab_size))\n",
    "\n",
    "    #Create placeholders for holding a batch of videos, captions and caption masks\n",
    "    video = tf.placeholder(tf.float32,shape=[batch_size,n_steps_vid,frame_dim],name='Input_Video')\n",
    "    caption = tf.placeholder(tf.int32,shape=[batch_size,n_steps_vocab],name='GT_Caption')\n",
    "    caption_mask = tf.placeholder(tf.float32,shape=[batch_size,n_steps_vocab],name='Caption_Mask')\n",
    "    dropout_prob = tf.placeholder(tf.float32,name='Dropout_Keep_Probability')\n",
    "\n",
    "    with tf.variable_scope('Im2Cap') as scope:\n",
    "        W_im2cap = tf.get_variable(name='W_im2cap',shape=[frame_dim,\n",
    "                                                    hidden_dim],\n",
    "                                                    initializer=tf.random_uniform_initializer(minval=-0.08,maxval=0.08))\n",
    "        b_im2cap = tf.get_variable(name='b_im2cap',shape=[hidden_dim],\n",
    "                                                    initializer=tf.constant_initializer(0.0))\n",
    "    with tf.variable_scope('Hid2Vocab') as scope:\n",
    "        W_H2vocab = tf.get_variable(name='W_H2vocab',shape=[hidden_dim,vocab_size],\n",
    "                                                         initializer=tf.random_uniform_initializer(minval=-0.08,maxval=0.08))\n",
    "        b_H2vocab = tf.Variable(name='b_H2vocab',initial_value=bias_init_vector.astype(np.float32))\n",
    "\n",
    "    with tf.variable_scope('Word_Vectors') as scope:\n",
    "        word_emb = tf.get_variable(name='Word_embedding',shape=[vocab_size,hidden_dim],\n",
    "                                                                initializer=tf.random_uniform_initializer(minval=-0.08,maxval=0.08))\n",
    "    print (\"Created weights\")\n",
    "\n",
    "    #Build two LSTMs, one for processing the video and another for generating the caption\n",
    "    with tf.variable_scope('LSTM_Video',reuse=None) as scope:\n",
    "        lstm_vid = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim)\n",
    "        lstm_vid = tf.nn.rnn_cell.DropoutWrapper(lstm_vid,output_keep_prob=dropout_prob)\n",
    "    with tf.variable_scope('LSTM_Caption',reuse=None) as scope:\n",
    "        lstm_cap = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim)\n",
    "        lstm_cap = tf.nn.rnn_cell.DropoutWrapper(lstm_cap,output_keep_prob=dropout_prob)\n",
    "\n",
    "    #Prepare input for lstm_video\n",
    "    video_rshp = tf.reshape(video,[-1,frame_dim])\n",
    "    video_rshp = tf.nn.dropout(video_rshp,keep_prob=dropout_prob)\n",
    "    video_emb = tf.nn.xw_plus_b(video_rshp,W_im2cap,b_im2cap)\n",
    "    video_emb = tf.reshape(video_emb,[batch_size,n_steps_vid,hidden_dim])\n",
    "    padding = tf.zeros([batch_size,2*n_steps_vocab-2,hidden_dim])\n",
    "    video_input = tf.concat([video_emb,padding],1)\n",
    "    #video_input=video_emb\n",
    "    print (\"Video_input: {}\".format(video_input.get_shape()))\n",
    "    #Run lstm_vid for 2*n_steps-1 timesteps\n",
    "    with tf.variable_scope('LSTM_Video') as scope:\n",
    "        out_vid,state_vid = tf.nn.dynamic_rnn(lstm_vid,video_input,dtype=tf.float32)\n",
    "    print (\"Video_output: {}\".format(out_vid.get_shape()))\n",
    "\n",
    "    #Prepare input for lstm_cap\n",
    "    padding = tf.zeros([batch_size,n_steps_vocab,hidden_dim])\n",
    "    caption_vectors = tf.nn.embedding_lookup(word_emb,caption[:,0:n_steps_vocab-1])\n",
    "    caption_vectors = tf.nn.dropout(caption_vectors,keep_prob=dropout_prob)\n",
    "    caption_2n = tf.concat([padding,caption_vectors],1)\n",
    "    #caption_2n = caption_vectors\n",
    "    caption_input = tf.concat([caption_2n,out_vid],2)\n",
    "    caption_input = caption_2n\n",
    "    print (\"Caption_input: {}\".format(caption_input.get_shape()))\n",
    "    #Run lstm_cap for 2*n_steps-1 timesteps\n",
    "    with tf.variable_scope('LSTM_Caption') as scope:\n",
    "        out_cap,state_cap = tf.nn.dynamic_rnn(lstm_cap,caption_input,dtype=tf.float32)\n",
    "    print (\"Caption_output: {}\".format(out_cap.get_shape()))\n",
    "\n",
    "    #Compute masked loss\n",
    "    output_captions = out_cap[:,n_steps_vocab:,:]\n",
    "    output_logits = tf.reshape(output_captions,[-1,hidden_dim])\n",
    "    output_logits = tf.nn.dropout(output_logits,keep_prob=dropout_prob)\n",
    "    output_logits = tf.nn.xw_plus_b(output_logits,W_H2vocab,b_H2vocab)\n",
    "    output_labels = tf.reshape(caption[:,1:],[-1])\n",
    "    caption_mask_out = tf.reshape(caption_mask[:,1:],[-1])\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output_logits,labels=output_labels)\n",
    "    masked_loss = loss*caption_mask_out\n",
    "    loss = tf.reduce_sum(masked_loss)/tf.reduce_sum(caption_mask_out)\n",
    "    return video,caption,caption_mask,output_logits,loss,dropout_prob\n",
    "\n",
    "db1 = None\n",
    "db2 = None\n",
    "db3 = None\n",
    "def train():\n",
    "    global db1,db2,db3\n",
    "    with tf.Graph().as_default():\n",
    "        learning_rate = 0.0001\n",
    "        video,caption,caption_mask,output_logits,loss,dropout_prob = build_model()\n",
    "        optim = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "        nEpoch = 300\n",
    "        nIter = int(nEpoch*60000/batch_size)\n",
    "        \n",
    "        ckpt_file = './ckpt_v5/model_58000.ckpt.meta'\n",
    "        ckpt_file = None\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            if ckpt_file:\n",
    "                saver_ = tf.train.import_meta_graph(ckpt_file)\n",
    "                saver_.restore(sess,'./ckpt_v5/model_58000.ckpt')\n",
    "                print (\"Restored model\")\n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            for i in range(nIter):\n",
    "                #print(i)\n",
    "                vids,caps,caps_mask = fetch_data_batch(batch_size=batch_size)\n",
    "                db1,db2,db3 = vids, caps, caps_mask\n",
    "                #print(type(vids),type(caps), type(caps_mask))\n",
    "                #print(vids,caps, caps_mask)\n",
    "                _,curr_loss,o_l = sess.run([optim,loss,output_logits],feed_dict={video:vids,\n",
    "                                                                            caption:caps,\n",
    "                                                                            caption_mask:caps_mask,\n",
    "                                                                            dropout_prob:0.5})\n",
    "\n",
    "                if i%1000 == 0:\n",
    "                    print (\"\\nIteration {} \\n\".format(i))\n",
    "                    out_logits = o_l.reshape([batch_size,n_steps_vocab-1,vocab_size])\n",
    "                    output_captions = np.argmax(out_logits,2)\n",
    "                    #print_in_english(output_captions[0:4])\n",
    "                    #print (\"GT Captions\")\n",
    "                    #print_in_english(caps[0:4])\n",
    "                    print (\"Current train loss: {} \".format(curr_loss))\n",
    "                    vids,caps,caps_mask,_ = fetch_data_batch_val(batch_size=batch_size)\n",
    "                    db1,db2,db3 = vids,caps,caps_mask\n",
    "                    curr_loss,o_l = sess.run([loss,output_logits],feed_dict={video:vids,\n",
    "                                                                            caption:caps,\n",
    "                                                                            caption_mask:caps_mask,\n",
    "                                                                            dropout_prob:1.0})\n",
    "                    out_logits = o_l.reshape([batch_size,n_steps_vocab-1,vocab_size])\n",
    "                    output_captions = np.argmax(out_logits,2)\n",
    "                    print_in_english(output_captions[0:2])\n",
    "                    print (\"GT Captions\")\n",
    "                    print_in_english(caps[0:2])\n",
    "                    print (\"Current validation loss: {} \".format(curr_loss))\n",
    "\n",
    "                if i%2000 == 0:\n",
    "                    saver.save(sess,'./ckpt_v6/model_'+str(i)+'.ckpt')\n",
    "                    print ('Saved {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Begins !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network config: \n",
      "N_Steps: 1\n",
      "Hidden_dim:500\n",
      "Frame_dim:4096\n",
      "Batch_size:1\n",
      "Vocab_size:29325\n",
      "\n",
      "Created weights\n",
      "Video_input: (1, 59, 500)\n",
      "Video_output: (1, 59, 500)\n",
      "Caption_input: (1, 59, 500)\n",
      "Caption_output: (1, 59, 500)\n",
      "\n",
      "Iteration 0 \n",
      "\n",
      "Current train loss: 5.362500190734863 \n",
      "1 a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a band of chipmunks covers a jackson five song\n",
      "..................................................\n",
      "Current validation loss: 6.7552385330200195 \n",
      "Saved 0\n",
      "\n",
      "Iteration 1000 \n",
      "\n",
      "Current train loss: 3.849146604537964 \n",
      "1 a a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man is giving a ted presentation\n",
      "..................................................\n",
      "Current validation loss: 4.515481948852539 \n",
      "\n",
      "Iteration 2000 \n",
      "\n",
      "Current train loss: 4.973325729370117 \n",
      "1 a man is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a  blue color car moving inside forest beside trees slowly displaying on screen\n",
      "..................................................\n",
      "Current validation loss: 6.329678535461426 \n",
      "Saved 2000\n",
      "\n",
      "Iteration 3000 \n",
      "\n",
      "Current train loss: 5.057857036590576 \n",
      "1 a man is a a a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman slicing small squid while explaining how to slice it\n",
      "..................................................\n",
      "Current validation loss: 6.09047269821167 \n",
      "\n",
      "Iteration 4000 \n",
      "\n",
      "Current train loss: 4.770488262176514 \n",
      "1 a is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> pictures of football players posing in the endzone\n",
      "..................................................\n",
      "Current validation loss: 6.28131628036499 \n",
      "Saved 4000\n",
      "\n",
      "Iteration 5000 \n",
      "\n",
      "Current train loss: 5.870491027832031 \n",
      "1 a is is a a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> meat being cooked in a pan\n",
      "..................................................\n",
      "Current validation loss: 5.26476526260376 \n",
      "\n",
      "Iteration 6000 \n",
      "\n",
      "Current train loss: 6.341740608215332 \n",
      "1 a is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> man wiring somethign electrical\n",
      "..................................................\n",
      "Current validation loss: 9.318536758422852 \n",
      "Saved 6000\n",
      "\n",
      "Iteration 7000 \n",
      "\n",
      "Current train loss: 4.915063858032227 \n",
      "1 a man is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a lovely lady is going to explain how to make a a veg and shrimp dish in lemon and garlic sauce \n",
      "..................................................\n",
      "Current validation loss: 6.191427707672119 \n",
      "\n",
      "Iteration 8000 \n",
      "\n",
      "Current train loss: 7.185211181640625 \n",
      "1 a man is a a a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a mason layer is working on building something\n",
      "..................................................\n",
      "Current validation loss: 6.292708873748779 \n",
      "Saved 8000\n",
      "\n",
      "Iteration 9000 \n",
      "\n",
      "Current train loss: 6.912484645843506 \n",
      "1 a man is a about a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a news reporter talking about a particular crime\n",
      "..................................................\n",
      "Current validation loss: 4.875232696533203 \n",
      "\n",
      "Iteration 10000 \n",
      "\n",
      "Current train loss: 5.8083109855651855 \n",
      "1 a is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> portal game short 3d animation\n",
      "..................................................\n",
      "Current validation loss: 7.624443054199219 \n",
      "Saved 10000\n",
      "\n",
      "Iteration 11000 \n",
      "\n",
      "Current train loss: 6.745952606201172 \n",
      "1 a are are are\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> two people are singing on the beach\n",
      "..................................................\n",
      "Current validation loss: 2.8204195499420166 \n",
      "\n",
      "Iteration 12000 \n",
      "\n",
      "Current train loss: 4.6840925216674805 \n",
      "1 a man is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> an advertisement for edible flowers\n",
      "..................................................\n",
      "Current validation loss: 6.9872589111328125 \n",
      "Saved 12000\n",
      "\n",
      "Iteration 13000 \n",
      "\n",
      "Current train loss: 5.25754976272583 \n",
      "1 a is a man is talking about a video\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> there is a man is talking about a meet dish\n",
      "..................................................\n",
      "Current validation loss: 2.604363203048706 \n",
      "\n",
      "Iteration 14000 \n",
      "\n",
      "Current train loss: 7.8207292556762695 \n",
      "1 a man of a are a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a group of men in business suits standing around and talking to each other\n",
      "..................................................\n",
      "Current validation loss: 4.623636245727539 \n",
      "Saved 14000\n",
      "\n",
      "Iteration 15000 \n",
      "\n",
      "Current train loss: 6.495982646942139 \n",
      "1 a is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> someone is applying the finishing touches of goth styled makeup to their face while looking in a hand held mirror the person is wearing a black wig\n",
      "..................................................\n",
      "Current validation loss: 6.236503601074219 \n",
      "\n",
      "Iteration 16000 \n",
      "\n",
      "Current train loss: 4.025001049041748 \n",
      "1 a are playing\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> people are controlling puppets who are talking to each other\n",
      "..................................................\n",
      "Current validation loss: 5.5757060050964355 \n",
      "Saved 16000\n",
      "\n",
      "Iteration 17000 \n",
      "\n",
      "Current train loss: 2.616590976715088 \n",
      "1 a are\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> men fighting in a war\n",
      "..................................................\n",
      "Current validation loss: 3.9784014225006104 \n",
      "\n",
      "Iteration 18000 \n",
      "\n",
      "Current train loss: 4.765532970428467 \n",
      "1 a man is a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man showcasing a vehicle part\n",
      "..................................................\n",
      "Current validation loss: 4.233736515045166 \n",
      "Saved 18000\n",
      "\n",
      "Iteration 19000 \n",
      "\n",
      "Current train loss: 3.8971900939941406 \n",
      "1 a man man is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a little girl surf in the water of the beach while the waves raising\n",
      "..................................................\n",
      "Current validation loss: 4.915351390838623 \n",
      "\n",
      "Iteration 20000 \n",
      "\n",
      "Current train loss: 4.980892181396484 \n",
      "1 a man is a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a person doing a cooking show and showing the ingredients for the recipe\n",
      "..................................................\n",
      "Current validation loss: 3.67372727394104 \n",
      "Saved 20000\n",
      "\n",
      "Iteration 21000 \n",
      "\n",
      "Current train loss: 6.468964576721191 \n",
      "1 a man is a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a box of the cage is shown and how to make a cage is explained\n",
      "..................................................\n",
      "Current validation loss: 4.325371742248535 \n",
      "\n",
      "Iteration 22000 \n",
      "\n",
      "Current train loss: 4.675454616546631 \n",
      "1 a men are a man is a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> two woman and a man give a talk on a tv show\n",
      "..................................................\n",
      "Current validation loss: 3.5811750888824463 \n",
      "Saved 22000\n",
      "\n",
      "Iteration 23000 \n",
      "\n",
      "Current train loss: 5.180557727813721 \n",
      "1 a man a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> the make up men telling their make up secret to their fans in very  simply\n",
      "..................................................\n",
      "Current validation loss: 6.502036094665527 \n",
      "\n",
      "Iteration 24000 \n",
      "\n",
      "Current train loss: 4.651417255401611 \n",
      "1 a man man is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a white bowl next to two cups of sauce with with white powder being poured into the white bowl\n",
      "..................................................\n",
      "Current validation loss: 5.697571754455566 \n",
      "Saved 24000\n",
      "\n",
      "Iteration 25000 \n",
      "\n",
      "Current train loss: 5.052845001220703 \n",
      "1 a man man man is a a in a a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a little blonde 8-year-old costa rican girl surfs on a small wave\n",
      "..................................................\n",
      "Current validation loss: 7.767542839050293 \n",
      "\n",
      "Iteration 26000 \n",
      "\n",
      "Current train loss: 1.2396544218063354 \n",
      "1 a man is a is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman in red patterned dress talks in a room inside of a home\n",
      "..................................................\n",
      "Current validation loss: 4.44080114364624 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26000\n",
      "\n",
      "Iteration 27000 \n",
      "\n",
      "Current train loss: 5.8509416580200195 \n",
      "1 a is is talking a video\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> this woman is giving a computer tutorial\n",
      "..................................................\n",
      "Current validation loss: 3.895998239517212 \n",
      "\n",
      "Iteration 28000 \n",
      "\n",
      "Current train loss: 2.0780346393585205 \n",
      "1 a man is talking a a about a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man is driving and talking to the camera between shots of the road and time lapse video of men arranging things items in the back of\n",
      "..................................................\n",
      "Current validation loss: 5.063015937805176 \n",
      "Saved 28000\n",
      "\n",
      "Iteration 29000 \n",
      "\n",
      "Current train loss: 3.0023443698883057 \n",
      "1 a men are about a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> two females talking about candy\n",
      "..................................................\n",
      "Current validation loss: 4.92094612121582 \n",
      "\n",
      "Iteration 30000 \n",
      "\n",
      "Current train loss: 7.137655735015869 \n",
      "1 a man is a man is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a chicken and a cat fighting\n",
      "..................................................\n",
      "Current validation loss: 4.585607051849365 \n",
      "Saved 30000\n",
      "\n",
      "Iteration 31000 \n",
      "\n",
      "Current train loss: 4.6316657066345215 \n",
      "1 a man of a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a cartoon describing one of the first video games created\n",
      "..................................................\n",
      "Current validation loss: 5.0966596603393555 \n",
      "\n",
      "Iteration 32000 \n",
      "\n",
      "Current train loss: 5.50925350189209 \n",
      "1 a man is talking about a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman is talking about the exterior of her car\n",
      "..................................................\n",
      "Current validation loss: 2.82004451751709 \n",
      "Saved 32000\n",
      "\n",
      "Iteration 33000 \n",
      "\n",
      "Current train loss: 3.1527726650238037 \n",
      "1 a are talking on the\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> people are working on a project in an office setting\n",
      "..................................................\n",
      "Current validation loss: 4.3138108253479 \n",
      "\n",
      "Iteration 34000 \n",
      "\n",
      "Current train loss: 4.496661186218262 \n",
      "1 a man is talking\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man is walking\n",
      "..................................................\n",
      "Current validation loss: 1.785221815109253 \n",
      "Saved 34000\n",
      "\n",
      "Iteration 35000 \n",
      "\n",
      "Current train loss: 2.7763442993164062 \n",
      "1 a man is a is talking to a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man and woman are speaking at a table\n",
      "..................................................\n",
      "Current validation loss: 2.541499376296997 \n",
      "\n",
      "Iteration 36000 \n",
      "\n",
      "Current train loss: 4.256447792053223 \n",
      "1 a man is a video\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man operates a phone while flower displays and such are passed by\n",
      "..................................................\n",
      "Current validation loss: 6.185863971710205 \n",
      "Saved 36000\n",
      "\n",
      "Iteration 37000 \n",
      "\n",
      "Current train loss: 5.931041240692139 \n",
      "1 a man of is talking a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a cartoon man is holding money\n",
      "..................................................\n",
      "Current validation loss: 3.4458165168762207 \n",
      "\n",
      "Iteration 38000 \n",
      "\n",
      "Current train loss: 3.9289050102233887 \n",
      "1 a man of are talking\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a couple people are commentating over the video game minecraft\n",
      "..................................................\n",
      "Current validation loss: 4.3126959800720215 \n",
      "Saved 38000\n",
      "\n",
      "Iteration 39000 \n",
      "\n",
      "Current train loss: 8.528098106384277 \n",
      "1 a man is talking\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man is cooking some dish in the kadai\n",
      "..................................................\n",
      "Current validation loss: 3.9117355346679688 \n",
      "\n",
      "Iteration 40000 \n",
      "\n",
      "Current train loss: 4.395867347717285 \n",
      "1 a man is a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman news anchor talks about a debate on fox news\n",
      "..................................................\n",
      "Current validation loss: 4.837018966674805 \n",
      "Saved 40000\n",
      "\n",
      "Iteration 41000 \n",
      "\n",
      "Current train loss: 4.532079219818115 \n",
      "1 a man is a kitchen is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> the woman in the tank top talk with the large trees behind her\n",
      "..................................................\n",
      "Current validation loss: 4.861266136169434 \n",
      "\n",
      "Iteration 42000 \n",
      "\n",
      "Current train loss: 3.072403907775879 \n",
      "1 a man man is about a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> an old lady speaking about children\n",
      "..................................................\n",
      "Current validation loss: 3.943246603012085 \n",
      "Saved 42000\n",
      "\n",
      "Iteration 43000 \n",
      "\n",
      "Current train loss: 5.820377349853516 \n",
      "1 a man man is a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a long pause of blank space beofore the inevidible conclusion\n",
      "..................................................\n",
      "Current validation loss: 9.713397026062012 \n",
      "\n",
      "Iteration 44000 \n",
      "\n",
      "Current train loss: 4.263298988342285 \n",
      "1 a man is\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a car commercial about the inside and outside\n",
      "..................................................\n",
      "Current validation loss: 4.119138240814209 \n",
      "Saved 44000\n",
      "\n",
      "Iteration 45000 \n",
      "\n",
      "Current train loss: 7.74439001083374 \n",
      "1 a man of a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a set of reporters provide political commentary\n",
      "..................................................\n",
      "Current validation loss: 6.667819976806641 \n",
      "\n",
      "Iteration 46000 \n",
      "\n",
      "Current train loss: 3.635146379470825 \n",
      "1 a of is a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> football player makes an amazing tackle\n",
      "..................................................\n",
      "Current validation loss: 6.798780918121338 \n",
      "Saved 46000\n",
      "\n",
      "Iteration 47000 \n",
      "\n",
      "Current train loss: 4.705246925354004 \n",
      "1 a man is a black shirt is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a model in a pink dress struts down the runway followed by another in black and white\n",
      "..................................................\n",
      "Current validation loss: 5.189020156860352 \n",
      "\n",
      "Iteration 48000 \n",
      "\n",
      "Current train loss: 5.512157440185547 \n",
      "1 a man is to a video\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman speaks in a classroom\n",
      "..................................................\n",
      "Current validation loss: 2.945960521697998 \n",
      "Saved 48000\n",
      "\n",
      "Iteration 49000 \n",
      "\n",
      "Current train loss: 5.072732448577881 \n",
      "1 a are playing\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> alligators are pursuing a man\n",
      "..................................................\n",
      "Current validation loss: 7.411179065704346 \n",
      "\n",
      "Iteration 50000 \n",
      "\n",
      "Current train loss: 5.050793170928955 \n",
      "1 a man is a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a person playing gta 5\n",
      "..................................................\n",
      "Current validation loss: 4.97804069519043 \n",
      "Saved 50000\n",
      "\n",
      "Iteration 51000 \n",
      "\n",
      "Current train loss: 7.125720500946045 \n",
      "1 a man is talking a car\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a man is doing a tutorial on plumbing\n",
      "..................................................\n",
      "Current validation loss: 3.549060344696045 \n",
      "\n",
      "Iteration 52000 \n",
      "\n",
      "Current train loss: 6.104312896728516 \n",
      "1 a man is a a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a lion attacking some deers\n",
      "..................................................\n",
      "Current validation loss: 7.409173488616943 \n",
      "Saved 52000\n",
      "\n",
      "Iteration 53000 \n",
      "\n",
      "Current train loss: 5.090949058532715 \n",
      "1 a is playing a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> someone is cooking dumplings\n",
      "..................................................\n",
      "Current validation loss: 4.079842567443848 \n",
      "\n",
      "Iteration 54000 \n",
      "\n",
      "Current train loss: 6.498948097229004 \n",
      "1 a man is a video\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a person peeling a potato then mashing it in a bowl\n",
      "..................................................\n",
      "Current validation loss: 5.104984760284424 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 54000\n",
      "\n",
      "Iteration 55000 \n",
      "\n",
      "Current train loss: 4.292474746704102 \n",
      "1 a are a in\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> 2015 long jump qualifiers listed on a slideshow from the website keinanbriggs\n",
      "..................................................\n",
      "Current validation loss: 8.346803665161133 \n",
      "\n",
      "Iteration 56000 \n",
      "\n",
      "Current train loss: 4.07150411605835 \n",
      "1 a man is a black shirt is is talking a to a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> a woman wearing a green floral shirt is applying makeup for online viewers\n",
      "..................................................\n",
      "Current validation loss: 4.722033500671387 \n",
      "Saved 56000\n",
      "\n",
      "Iteration 57000 \n",
      "\n",
      "Current train loss: 3.0948379039764404 \n",
      "1 a are a\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> kids throws football at target\n",
      "..................................................\n",
      "Current validation loss: 6.082596302032471 \n",
      "\n",
      "Iteration 58000 \n",
      "\n",
      "Current train loss: 7.242854118347168 \n",
      "1 a is a man game a man\n",
      "..................................................\n",
      "GT Captions\n",
      "1 <BOS> it is a video about a guy saying how he wants a normal life and to change\n",
      "..................................................\n",
      "Current validation loss: 5.169660568237305 \n",
      "Saved 58000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-fa732bbd7de2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m                                                                             \u001b[0mcaption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                                                                             \u001b[0mcaption_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcaps_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                                                                             dropout_prob:0.5})\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network config: \n",
      "N_Steps: 1\n",
      "Hidden_dim:500\n",
      "Frame_dim:4096\n",
      "Batch_size:1\n",
      "Vocab_size:29325\n",
      "\n",
      "Created weights\n",
      "Video_input: (1, 59, 500)\n",
      "Video_output: (1, 59, 500)\n",
      "Caption_input: (1, 59, 500)\n",
      "Caption_output: (1, 59, 500)\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_v5/model_58000.ckpt\n",
      "Restored model\n",
      "1 <BOS> a\n",
      "..................................................\n",
      "1 <BOS> a man\n",
      "..................................................\n",
      "1 <BOS> a man is\n",
      "..................................................\n",
      "1 <BOS> a man is talking\n",
      "..................................................\n",
      "1 <BOS> a man is talking about\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a man\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a man\n",
      "..................................................\n",
      "............................\n",
      "GT Caption:\n",
      "\n",
      "1 <BOS> a woman is showing how to peel a potato\n",
      "..................................................\n",
      "Should I play the video? n\n",
      "Want another test run? y\n",
      "1 <BOS> a\n",
      "..................................................\n",
      "1 <BOS> a man\n",
      "..................................................\n",
      "1 <BOS> a man is\n",
      "..................................................\n",
      "1 <BOS> a man is talking\n",
      "..................................................\n",
      "1 <BOS> a man is talking about\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a man\n",
      "..................................................\n",
      "1 <BOS> a man is talking about a man\n",
      "..................................................\n",
      "............................\n",
      "GT Caption:\n",
      "\n",
      "1 <BOS> ricki lake thanking the crowd\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    with tf.Graph().as_default():\n",
    "        learning_rate = 0.00001\n",
    "        video,caption,caption_mask,output_logits,loss,dropout_prob = build_model()\n",
    "        optim = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "        ckpt_file = './ckpt_v5/model_58000.ckpt.meta'\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            if ckpt_file:\n",
    "                saver_ = tf.train.import_meta_graph(ckpt_file)\n",
    "                saver_.restore(sess,'./ckpt_v5/model_58000.ckpt')\n",
    "                print (\"Restored model\")\n",
    "            else:\n",
    "                sess.run(tf.initialize_all_variables())\n",
    "            while(1):\n",
    "                vid,caption_GT,_,current_batch_vids = fetch_data_batch_val(1)\n",
    "                caps,caps_mask = convert_caption(['<BOS>'],word2id,30)\n",
    "\n",
    "                for i in range(30):\n",
    "                    o_l = sess.run(output_logits,feed_dict={video:vid,\n",
    "                                                            caption:caps,\n",
    "                                                            caption_mask:caps_mask,\n",
    "                                                            dropout_prob:1.0})\n",
    "                    out_logits = o_l.reshape([batch_size,n_steps_vocab-1,vocab_size])\n",
    "                    output_captions = np.argmax(out_logits,2)\n",
    "                    caps[0][i+1] = output_captions[0][i]\n",
    "                    print_in_english(caps)\n",
    "                    if id2word[output_captions[0][i]] == '<EOS>':\n",
    "                        break\n",
    "                print ('............................\\nGT Caption:\\n')\n",
    "                print_in_english(caption_GT)\n",
    "                play_video = input('Should I play the video? ')\n",
    "                if play_video.lower() == 'y':\n",
    "                    playVideo(current_batch_vids)\n",
    "                test_again = input('Want another test run? ')\n",
    "                if test_again.lower() == 'n':\n",
    "                    break\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New 3D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(vocab_len, sent):\n",
    "    nsent = []\n",
    "    for i in range(len(sent)):\n",
    "        one_hot_vector = np.zeros(vocab_len)\n",
    "        one_hot_vector[sent[i]] = 1\n",
    "        nsent.append(one_hot_vector)\n",
    "    return nsent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def __init__(self):\n",
    "        self.max_cap_len = 10\n",
    "        self.vocab_size = len(word2id)\n",
    "        self.EMBEDDING_DIM = 500\n",
    "        self.epochs = 300\n",
    "        self.batch_size = 100\n",
    "        self.niter = 600*self.epochs/self.batch_size\n",
    "\n",
    "    def create_model(self, ret_model = False):\n",
    "\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Dense(self.EMBEDDING_DIM, input_dim = 4096, activation='relu', name='imdense'))\n",
    "\n",
    "        image_model.add(RepeatVector(self.max_cap_len, name='imrepeat'))\n",
    "\n",
    "        lang_model = Sequential()\n",
    "        lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len, name='lang_embedding'))\n",
    "        lang_model.add(LSTM(256,return_sequences=True))\n",
    "        lang_model.add(TimeDistributed(Dense(self.EMBEDDING_DIM)))\n",
    "        #print(lang_model.summary())\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
    "        model.add(LSTM(1000,return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(self.vocab_size)))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        print (\"Model created!\")\n",
    "\n",
    "        if(ret_model==True):\n",
    "            return model\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        def generator():\n",
    "            while True:\n",
    "                vids,caps,caps_mask = fetch_data_batch(batch_size=self.batch_size)\n",
    "                caps_hot = np.array([one_hot(self.vocab_size, caps[i]) for i in range(self.batch_size)])\n",
    "           \n",
    "                yield [[vids,caps],np.array(caps_hot)]\n",
    "            \n",
    "        cg = Caption_Generator()\n",
    "        model = cg.create_model()\n",
    "        #print(model.summary())\n",
    "        model.fit_generator(generator(),steps_per_epoch=int(6000/self.batch_size), epochs=10, max_queue_size=2)\n",
    "        return model\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 28s 465ms/step - loss: 4.4184 - acc: 0.4139\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 2.9200 - acc: 0.5472\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 26s 441ms/step - loss: 2.5029 - acc: 0.5852\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 27s 442ms/step - loss: 2.2576 - acc: 0.6152\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 27s 443ms/step - loss: 2.1155 - acc: 0.6353\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 26s 438ms/step - loss: 2.0506 - acc: 0.6477\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 26s 439ms/step - loss: 2.0014 - acc: 0.6535\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 27s 443ms/step - loss: 1.9359 - acc: 0.6641\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 1.8940 - acc: 0.6681\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 26s 440ms/step - loss: 1.8438 - acc: 0.6773\n"
     ]
    }
   ],
   "source": [
    "vcg = Caption_Generator()\n",
    "model = vcg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "vid,caption_GT,_,current_batch_vids = fetch_data_batch_val(1)\n",
    "caps,caps_mask = convert_caption(['<BOS>'],word2id,30)\n",
    "caps_hot = np.array([one_hot(len(word2id), caps[i]) for i in range(1)])\n",
    "print(len(caps_hot[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected lang_embedding_input to have shape (None, 10) but got array with shape (1, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fff21061e924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1764\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected lang_embedding_input to have shape (None, 10) but got array with shape (1, 30)"
     ]
    }
   ],
   "source": [
    "output = model.predict([np.array(vid),np.array(caps)], batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(arr, id2word):\n",
    "    '''\n",
    "    converts one hot encoded array to words \n",
    "    '''\n",
    "    m = arr.shape[0]\n",
    "    n = arr.shape[1]\n",
    "    n_arr = []\n",
    "    for i in range(m):\n",
    "        sent = []\n",
    "        for j in range(n):\n",
    "            sent.append(id2word[np.argmax(arr[i][j])])\n",
    "        n_arr.append(sent)\n",
    "    return n_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   1  58  19   1 145   7  30   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = reverse_one_hot(output,id2word)\n",
    "gold = reverse_one_hot(caption_GT,id2word)\n",
    "print(caption_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<BOS>', 'kids', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  3.74253117e-09   2.54695340e-08   9.99995947e-01 ...,   4.92554331e-11\n",
      "     5.17315149e-11   4.36999499e-11]\n",
      "  [  1.27241101e-05   2.01349201e-10   5.16931813e-08 ...,   3.22197447e-09\n",
      "     3.41369910e-09   2.80052270e-09]\n",
      "  [  9.96666491e-01   8.33321231e-16   1.15090346e-08 ...,   3.64110868e-12\n",
      "     4.35671377e-12   4.57362403e-12]\n",
      "  ..., \n",
      "  [  9.99958873e-01   1.38061066e-15   5.94136018e-10 ...,   3.04800889e-13\n",
      "     3.86122530e-13   4.14932222e-13]\n",
      "  [  9.99958396e-01   1.39722521e-15   5.95246241e-10 ...,   3.09059039e-13\n",
      "     3.93019872e-13   4.21471045e-13]\n",
      "  [  9.99957919e-01   1.39567429e-15   5.86061089e-10 ...,   3.11578508e-13\n",
      "     3.98518051e-13   4.26282897e-13]]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <BOS> a guy playing a guitar and singing\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "print_in_english(caption_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not numpy.str_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9905af4da1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_in_english\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-5c409808b921>\u001b[0m in \u001b[0;36mprint_in_english\u001b[0;34m(caption_idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"Function to take a list of captions with words mapped to ids and\n\u001b[1;32m     65\u001b[0m         print the captions after mapping word indices back to words.\"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mcaptions_english\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_english\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'<EOS>'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5c409808b921>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"Function to take a list of captions with words mapped to ids and\n\u001b[1;32m     65\u001b[0m         print the captions after mapping word indices back to words.\"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mcaptions_english\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_english\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'<EOS>'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5c409808b921>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"Function to take a list of captions with words mapped to ids and\n\u001b[1;32m     65\u001b[0m         print the captions after mapping word indices back to words.\"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mcaptions_english\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_english\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'<EOS>'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.str_"
     ]
    }
   ],
   "source": [
    "print_in_english(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<BOS>', 'kids', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>']]\n",
      "[['<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(gold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
