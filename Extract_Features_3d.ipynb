{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from MSR-VTT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = './videos'\n",
    "output_dir = './DATA/frames'\n",
    "features_out = './DATA/features'\n",
    "jobs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and wrote 0 video files.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "from subprocess import call\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "data_file = []\n",
    "\n",
    "def core_func(video_path):\n",
    "    global data_file\n",
    "    video_parts = get_video_parts(video_path)\n",
    "    filename_no_ext, filename = video_parts\n",
    "\n",
    "    # Only extract if we haven't done it yet. Otherwise, just get\n",
    "    # the info.\n",
    "    if not check_already_extracted(video_parts):\n",
    "        # Now extract it.\n",
    "        src = os.path.join(video_dir,filename)\n",
    "        dest = os.path.join(output_dir,filename_no_ext + '-%04d.jpg')\n",
    "        #print('in', src, dest)\n",
    "        call([\"ffmpeg\", \"-i\", src,\"-r\", \"4\", dest])\n",
    "    # Now get how many frames it is.\n",
    "    nb_frames = get_nb_frames_for_video(video_parts)\n",
    "    #print('written: ',nb_frames)\n",
    "    data_file.append([filename_no_ext, nb_frames])\n",
    "\n",
    "\n",
    "\n",
    "def get_nb_frames_for_video(video_parts):\n",
    "    \"\"\"Given video parts of an (assumed) already extracted video, return\n",
    "    the number of frames that were extracted.\"\"\"\n",
    "    filename_no_ext, _ = video_parts\n",
    "    generated_files = glob.glob(os.path.join(output_dir, filename_no_ext + '*.jpg'))\n",
    "    return len(generated_files)\n",
    "\n",
    "def get_video_parts(video_path):\n",
    "    \"\"\"Given a full path to a video, return its parts.\"\"\"\n",
    "    parts = video_path.split(os.path.sep)\n",
    "    #print(parts)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    return filename_no_ext, filename\n",
    "\n",
    "def check_already_extracted(video_parts):\n",
    "    \"\"\"Check to see if we created the -0001 frame of this file.\"\"\"\n",
    "    filename_no_ext, _ = video_parts\n",
    "    return bool(os.path.exists(os.path.join(output_dir,\n",
    "                               filename_no_ext + '-0001.jpg')))\n",
    "\n",
    "\n",
    "\n",
    "vfiles = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "results = Parallel(n_jobs=jobs)(delayed(core_func)(video_path) for video_path in vfiles)               \n",
    "\n",
    "with open('data_file.csv', 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerows(data_file)\n",
    "\n",
    "print(\"Extracted and wrote %d video files.\" % (len(data_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7351/7351 [1:08:43<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and wrote 7351 video files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data_file above did not work in parallel processing need to write a code to count them manually:\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=len(vfiles))\n",
    "\n",
    "data_file = []\n",
    "for video_path in vfiles:\n",
    "    video_parts = get_video_parts(video_path)\n",
    "    filename_no_ext, filename = video_parts\n",
    "    generated_files = glob.glob(os.path.join(output_dir, filename_no_ext + '*.jpg'))\n",
    "    data_file.append([filename_no_ext, len(generated_files)])\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "with open('data_file.csv', 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerows(data_file)\n",
    "\n",
    "print(\"Extracted and wrote %d video files.\" % (len(data_file)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video0.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!ls DATA/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for managing our data.\n",
    "\"\"\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import operator\n",
    "import threading\n",
    "from keras.utils import np_utils\n",
    "\n",
    "class threadsafe_iterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    \"\"\"Decorator\"\"\"\n",
    "    def gen(*a, **kw):\n",
    "        return threadsafe_iterator(func(*a, **kw))\n",
    "    return gen\n",
    "\n",
    "class DataSet():\n",
    "\n",
    "    def __init__(self, seq_length=40, class_limit=None, image_shape=(224, 224, 3)):\n",
    "        \"\"\"Constructor.\n",
    "        seq_length = (int) the number of frames to consider\n",
    "        class_limit = (int) number of classes to limit the data to.\n",
    "            None = no limit.\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        self.class_limit = class_limit\n",
    "        self.sequence_path = os.path.join('data', 'sequences')\n",
    "        self.max_frames = 1000  # max number of frames a video can have for us to use it\n",
    "\n",
    "        # Get the data.\n",
    "        self.data = self.get_data()\n",
    "\n",
    "        # Now do some minor data cleaning.\n",
    "        self.data = self.clean_data()\n",
    "\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        \"\"\"Load our data from file.\"\"\"\n",
    "        with open(os.path.join('data_file.csv'), 'r') as fin:\n",
    "            reader = csv.reader(fin)\n",
    "            data = list(reader)\n",
    "            #print(len(data))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Limit samples to greater than the sequence length and fewer\n",
    "        than N frames. Also limit it to classes we want to use.\"\"\"\n",
    "        data_clean = []\n",
    "        for item in self.data:\n",
    "            if int(item[1]) >= self.seq_length and int(item[1]) <= self.max_frames:\n",
    "                data_clean.append(item)\n",
    "\n",
    "        return data_clean\n",
    "\n",
    "\n",
    "    def build_image_sequence(self, frames):\n",
    "        \"\"\"Given a set of frames (filenames), build our sequence.\"\"\"\n",
    "        return [process_image(x, self.image_shape) for x in frames]\n",
    "\n",
    "    def get_extracted_sequence(self, data_type, sample):\n",
    "        \"\"\"Get the saved extracted features.\"\"\"\n",
    "        filename = sample[2]\n",
    "        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n",
    "            '-' + data_type + '.npy')\n",
    "        if os.path.isfile(path):\n",
    "            return np.load(path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_for_sample(sample):\n",
    "        \"\"\"Given a sample row from the data file, get all the corresponding frame\n",
    "        filenames.\"\"\"\n",
    "        filename = sample[0]\n",
    "        images = sorted(glob.glob(os.path.join(output_dir, filename + '*jpg')))\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filename_from_image(filename):\n",
    "        parts = filename.split(os.path.sep)\n",
    "        return parts[-1].replace('.jpg', '')\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_list(input_list, size):\n",
    "        \"\"\"Given a list and a size, return a rescaled/samples list. For example,\n",
    "        if we want a list of size 5 and we have a list of size 25, return a new\n",
    "        list of size five which is every 5th element of the origina list.\"\"\"\n",
    "        assert len(input_list) >= size\n",
    "\n",
    "        # Get the number to skip between iterations.\n",
    "        skip = len(input_list) // size\n",
    "\n",
    "        # Build our new output.\n",
    "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "\n",
    "        # Cut off the last one if needed.\n",
    "        return output[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "\n",
    "def process_image(image, target_shape):\n",
    "    \"\"\"Given an image, process it and return the array.\"\"\"\n",
    "    # Load the image.\n",
    "    h, w, _ = target_shape\n",
    "    image = load_img(image, target_size=(h, w))\n",
    "\n",
    "    # Turn it into numpy, normalize and return.\n",
    "    img_arr = img_to_array(image)\n",
    "    x = (img_arr / 255.).astype(np.float32)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "\n",
    "class Extractor():\n",
    "    def __init__(self, weights=None):\n",
    "        \"\"\"Either load pretrained from imagenet, or load our saved\n",
    "        weights from our own training.\"\"\"\n",
    "\n",
    "        self.weights = weights  # so we can check elsewhere which model\n",
    "\n",
    "        if weights is None:\n",
    "            # Get model with pretrained weights.\n",
    "            base_model = InceptionV3(\n",
    "                weights='imagenet',\n",
    "                include_top=True\n",
    "            )\n",
    "\n",
    "            # We'll extract features at the final pool layer.\n",
    "            self.model = Model(\n",
    "                inputs=base_model.input,\n",
    "                outputs=base_model.get_layer('avg_pool').output\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Load the model first.\n",
    "            self.model = load_model(weights)\n",
    "\n",
    "            # Then remove the top so we get features not predictions.\n",
    "            # From: https://github.com/fchollet/keras/issues/2371\n",
    "            self.model.layers.pop()\n",
    "            self.model.layers.pop()  # two pops to get to pool layer\n",
    "            self.model.outputs = [self.model.layers[-1].output]\n",
    "            self.model.output_layers = [self.model.layers[-1]]\n",
    "            self.model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    def extract(self, image_path):\n",
    "        img = image.load_img(image_path, target_size=(299, 299))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Get the prediction.\n",
    "        features = self.model.predict(x)\n",
    "\n",
    "        if self.weights is None:\n",
    "            # For imagenet/default network:\n",
    "            features = features[0]\n",
    "        else:\n",
    "            # For loaded network:\n",
    "            features = features[0]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/7273 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7273/7273 [4:23:07<00:00,  2.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set defaults.\n",
    "seq_length = 30\n",
    "class_limit = None  # Number of classes to extract. Can be 1-101 or None for all.\n",
    "\n",
    "# Get the dataset.\n",
    "data = DataSet(seq_length=seq_length, class_limit=class_limit)\n",
    "\n",
    "# get the model.\n",
    "model = Extractor()\n",
    "\n",
    "print(len(data.data))\n",
    "# Loop through data.\n",
    "pbar = tqdm(total=len(data.data))\n",
    "for video in data.data:\n",
    "\n",
    "    # Get the path to the sequence for this video.\n",
    "    path = os.path.join(features_out, video[0] + '-' + str(seq_length) + \\\n",
    "        '-features')  # numpy will auto-append .npy\n",
    "\n",
    "    # Check if we already have it.\n",
    "    if os.path.isfile(path):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Get the frames for this video.\n",
    "    frames = data.get_frames_for_sample(video)\n",
    "\n",
    "    # Now downsample to just the ones we need.\n",
    "    frames = data.rescale_list(frames, seq_length)\n",
    "\n",
    "    # Now loop through and extract features to build the sequence.\n",
    "    sequence = []\n",
    "    for frame in frames:\n",
    "        features = model.extract(frame)\n",
    "        sequence.append(features)\n",
    "\n",
    "    # Save the sequence.\n",
    "    np.save(path, sequence)\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract 3D CNN features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "class Extractor_3d():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_dir = '/home/narain.adithya/c3d-keras/models'\n",
    "        self.model_weight_filename = os.path.join(model_dir, 'sports1M_weights_tf.h5')\n",
    "        self.model_json_filename = os.path.join(model_dir, 'sports1M_weights_tf.json')\n",
    "\n",
    "        self.model = model_from_json(open(model_json_filename, 'r').read())\n",
    "        self.model.layers.pop()\n",
    "        self.model.layers.pop()\n",
    "        self.model.layers.pop()\n",
    "        self.model.layers.pop()\n",
    "        self.model.outputs = [self.model.layers[-1].output]\n",
    "        self.model.output_layers = [self.model.layers[-1]]\n",
    "        self.model.layers[-1].outbound_nodes = []\n",
    "        #self.model.summary()\n",
    "        \n",
    "    def reshape(self, image_path):\n",
    "        img = image.load_img(image_path, target_size=(112, 112))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "    \n",
    "    def extract(self,x):\n",
    "        \n",
    "        # Get the prediction.\n",
    "        features = self.model.predict(x)\n",
    "\n",
    "        if self.weights is None:\n",
    "            # For imagenet/default network:\n",
    "            features = features[0]\n",
    "        else:\n",
    "            # For loaded network:\n",
    "            features = features[0]\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/7273 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "(1, 16, 112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set defaults.\n",
    "seq_length = 16\n",
    "class_limit = None  # Number of classes to extract. Can be 1-101 or None for all.\n",
    "\n",
    "# Get the dataset.\n",
    "data = DataSet(seq_length=seq_length, class_limit=class_limit)\n",
    "\n",
    "# get the model.\n",
    "model = Extractor_3d()\n",
    "\n",
    "print(len(data.data))\n",
    "# Loop through data.\n",
    "pbar = tqdm(total=len(data.data))\n",
    "for video in data.data:\n",
    "\n",
    "    # Get the path to the sequence for this video.\n",
    "    path = os.path.join(features_out, video[0] + '-' + str(seq_length) + \\\n",
    "        '-3dfeatures')  # numpy will auto-append .npy\n",
    "\n",
    "    # Check if we already have it.\n",
    "    if os.path.isfile(path):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Get the frames for this video.\n",
    "    frames = data.get_frames_for_sample(video)\n",
    "    print(len(frames))\n",
    "\n",
    "    # Now downsample to just the ones we need.\n",
    "    frames = data.rescale_list(frames, seq_length)\n",
    "\n",
    "    # Now loop through and extract features to build the sequence.\n",
    "    imgs = []\n",
    "    for frame in frames:\n",
    "        x = model.reshape(frame)\n",
    "        x = x[0]\n",
    "        imgs.append(x)\n",
    "        \n",
    "    x = []\n",
    "    x.append(imgs)\n",
    "    x = np.array(x)\n",
    "    print(x.shape)\n",
    "    feature = model.extract(x)\n",
    "    # Save the sequence.\n",
    "    np.save(path, feature)\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
